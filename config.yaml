# Global settings
settings:
  timeout: 30  # Request timeout in seconds 

# Primary backend configurations for LLM API endpoints
primary_backends:
  # First backend configuration
  - name: LLM1  # Identifier for the first backend
    url: https://api.openai.com/v1  # API endpoint URL
    model: "gpt-4o-mini"  # Model to use
  
  # Second backend configuration
  - name: LLM2  # Identifier for the second backend
    url: https://api.openai.com/v1  # API endpoint URL
    model: "gpt-4o-mini"  # Model to use

  # Third backend configuration
  - name: LLM3  # Identifier for the third backend
    url: https://api.openai.com/v1  # API endpoint URL
    model: "gpt-4o-mini"  # Model to use

iterations:
    aggregation:
      strategy: aggregate

strategy:
  # --- Concatenate Strategy ---
  # Simple strategy to join all backend responses together sequentially.
  concatenate:
    # String used to separate responses when skip_final_aggregation is false.
    separator: "\n\nSEPARATOR\n\n"
    # If true, hides thinking blocks from intermediate responses.
    hide_intermediate_think: false
    # If true, hides thinking blocks from the final combined response.
    hide_final_think: false
    # XML-like tags identifying blocks containing thought processes.
    thinking_tags: ["think", "reason", "reasoning", "thought", "Thought"]
    # If true, outputs are streamed/returned individually without using the separator.
    # If false, outputs are joined by the separator into one block.
    skip_final_aggregation: true

  # --- Aggregate Strategy ---
  # Strategy to send all initial responses to a final LLM for synthesis.
  aggregate:
    # Specifies which backends to use as input sources for aggregation
    # Options:
    # - "all": Use all primary_backends (default if omitted)
    # - List of specific backend names to include
    source_backends: ["LLM1", "LLM2", "LLM3"]
    # Alternative examples:
    # source_backends: "all"  # Use all backends
    # source_backends: ["LLM1", "LLM3"]  # Only use specific backends
    
    # The backend that will synthesize all source outputs
    # Must be defined in primary_backends
    aggregator_backend: "LLM1"
    
    # How to combine source outputs in the aggregator prompt
    intermediate_separator: "\n\n---\n\n"
    
    # Whether to include source backend names with their responses
    include_source_names: true
    
    # Format for labeling each source response when include_source_names is true
    source_label_format: "Response from {backend_name}:\n"
    
    # The prompt template sent to the aggregator backend
    prompt_template: |
      You have received the following responses regarding the user's query:
      
      {{intermediate_results}}
      
      Synthesize these responses into a single, comprehensive answer that captures
      the best information and insights from all sources. Resolve any contradictions
      and provide a coherent, unified response.
    
    # If true, removes thinking blocks from source responses before aggregation
    strip_intermediate_thinking: true
    
    # Controls visibility of the aggregator's thinking process in final output
    hide_aggregator_thinking: true
    
    # Tags identifying thinking blocks for both source and aggregator responses
    thinking_tags: ["think", "reason", "reasoning", "thought", "Thought"]
    
    # Whether to include the original user query in the aggregator prompt
    include_original_query: true
    
    # Format for including the original query when include_original_query is true
    query_format: "Original query: {query}\n\n"